{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4685e4",
   "metadata": {},
   "source": [
    "# Content-based Recommendation System with Python\n",
    "\n",
    "### The Overall Concept/Direction of this Recommendation System\n",
    "\n",
    "Here is the overall concept:\n",
    "\n",
    "1. Prepare a list of keywords (or `Bags Of Words(BOWs)`) from text content\n",
    "1. Count the occurences of each words using `CountVectorizer()`\n",
    "1. Use the output from above and calculate the similarities using `cosine_similarity()`\n",
    "1. Create a function to return the top `N` similar topics using the topic/title as input\n",
    "\n",
    "Reference: https://www.kaggle.com/code/annalee7/content-based-movie-recommendation-engine/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fd87e",
   "metadata": {},
   "source": [
    "#### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848ddc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install paddlepaddle\n",
    "# !pip install pandas numpy\n",
    "# !pip install sklearn\n",
    "# !pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d469d9",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbba39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes a collection of words extracted from a text doc\n",
    "import jieba\n",
    "import paddle\n",
    "import datetime\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import os\n",
    "import jieba.analyse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4fa5f",
   "metadata": {},
   "source": [
    "## Prepare a list of keywords (or `Bags Of Words(BOWs)`) from text content\n",
    "\n",
    "### Points to note:\n",
    "\n",
    "- This is actually the most time-consuming step, 80% of development time will be spent on this\n",
    "- This is just one way of preparing content, **this is not the only way!!!!!**\n",
    "\n",
    "### Remember, our main objective is to get a list of keywords (BOWs) for each article\n",
    "\n",
    "Here is how:\n",
    "\n",
    "- To get BOWs, we need to extract keywords from `post_title`, `post_content`, `post_category`, `post_tags`\n",
    "\n",
    "  - We use [`jieba`](https://github.com/fxsjy/jieba) to analyse and extract (or called **Tokenize**) the whole article into  keywords\n",
    "  - You can provide your own text list (called `userDict` in `jieba`) to specify your own keywords\n",
    "  - You can also use `jieba.add_word()` to provide your own keywords, I use both here. \n",
    "\n",
    "- __**In this case**__, BOW contains 2 parts\n",
    "\n",
    "  1. Directly use `post_category` and `post_tags` as keywords\n",
    "  1. Combine `post_title` and `post_content` together and use `jieba.analyse.extract_tags()` and `jieba.analyse.textrank()` to get a list of keywords\n",
    "    - [`jieba.analyse.extract_tags()`](https://github.com/fxsjy/jieba#%E5%9F%BA%E4%BA%8E-tf-idf-%E7%AE%97%E6%B3%95%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96) - Using TF-IDF (Find the word frequency and weight in an article) to extract top N keywords\n",
    "    - [`jieba.analyse.textrank()`](https://github.com/fxsjy/jieba#%E5%9F%BA%E4%BA%8E-textrank-%E7%AE%97%E6%B3%95%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96) - Use TextRank algorithm to extract top N keywords.\n",
    "\n",
    "\n",
    "- Once you have above, combine/join/merge (or however you called it) into BOW **for each article(row in dataFrame)**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eea25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"search.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c98195",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(data[\"hits\"][\"hits\"])\n",
    "# all_data[['post_id', 'post_title', 'post_content', 'post_type', 'post_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['keywords'] = all_data[[\"post_category\", \"post_tags\"]].apply(lambda x: x['post_category'] + x['post_tags'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e4b5a",
   "metadata": {},
   "source": [
    "Create a unique list (or `set()`) of keywords from `post_category` and `post_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e82532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = []\n",
    "for list_item in all_data['keywords']:\n",
    "    full_list += list_item\n",
    "\n",
    "categories_and_tags = set(full_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9e29e",
   "metadata": {},
   "source": [
    "Find missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3b58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.isnull().sum().to_frame()\n",
    "# all_data[['keywords', 'post_category', 'post_tags']].isnull().sum().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06531159",
   "metadata": {},
   "source": [
    "Join `post_title` and `post_content` together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e156451",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['title_content'] = pd.DataFrame(all_data['post_title'] + all_data['post_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0acef",
   "metadata": {},
   "source": [
    "Clean up the content by replacing/removing full-width text and unwanted text\n",
    "\n",
    "**Yours may be different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910d5e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4.36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"special_chars.txt\", encoding=\"utf8\") as f:\n",
    "    stop_words = list(f.read())\n",
    "\n",
    "def replace_words(text, stop_words):\n",
    "    for s in stop_words:\n",
    "        if(s in text):\n",
    "            text = text.replace(s, ' ')\n",
    "    \n",
    "    text = text.replace(\"延伸閱讀\", '') \\\n",
    "                .replace(\"\\xa0\", ' ') \\\n",
    "                .replace(\"  \", ' ')\n",
    "                \n",
    "    return text\n",
    "\n",
    "all_data['title_content'] = all_data['title_content'].apply(replace_words, args=(stop_words,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83619971",
   "metadata": {},
   "source": [
    "We are performing the same steps for both `post_title` and `post_content`\n",
    "\n",
    "- First we are going to use `jieba` to find keywords.\n",
    "  - To use `jieba`, you need to set it up.\n",
    "\n",
    "### What is `jieba.enable_paddle()`?  What is Paddle?\n",
    "\n",
    "[PaddlePaddle](https://github.com/PaddlePaddle) is a general machine learning library.  If you know TensorFlow, Keras or PyTorch, you can think of it as a similar \"Framework\".\n",
    "\n",
    "In order for `jieba` to tokenize text, there are two ways to do it:\n",
    "\n",
    "1. Provide your own list of text/keywords (or called \"Dictionary\" in terms of `jieba`)\n",
    "1. Use existing, public keywords list\n",
    "\n",
    "However, as the world is evolving and new texts are coming, it is better to use a machine-learning model, which is trained by others, to \"understand\" your content.\n",
    "\n",
    "To do that, `jieba` has a new feature that allow user to use Paddle to \"understand\" and tokenize your content.  All you need is to run `jieba.enable_paddle()` to enable that.  That's why you need to install `paddlepaddle` in the first place.\n",
    "\n",
    "This feature is only available starting from `jieba 0.4`.  Make sure you are using the right version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ac9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG 2022-06-30 12:41:16,736 _compat.py:47] Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "# You need to run this line before running `jieba.enable_paddle()`, or you will see error\n",
    "paddle.enable_static() \n",
    "jieba.enable_paddle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca7e3a",
   "metadata": {},
   "source": [
    "Then, We use `jieba.add_word()` to add a list of keywords (coming from `categories_and_tags`) as a custom list of specific keywords used by `jieba` to analyse (aka Tokenize) text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ecb9e",
   "metadata": {},
   "source": [
    "### What is `freq=5`?  Why are you doing this?\n",
    "\n",
    "I add it here because I just think that `post_category` and `post_tag` are, in this case, entered by our content editor, and hence it should have a higher weight (or `freq`, i.e. more important), so I set it here.\n",
    "\n",
    "This is optional (i.e. you can just use `jieba.add_word(w)`), you don't need to do this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f2acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG 2022-06-30 12:41:16,746 __init__.py:113] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\elleryl\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG 2022-06-30 12:41:16,747 __init__.py:132] Loading model from cache C:\\Users\\elleryl\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.598 seconds.\n",
      "DEBUG 2022-06-30 12:41:17,344 __init__.py:164] Loading model cost 0.598 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG 2022-06-30 12:41:17,346 __init__.py:166] Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "for w in categories_and_tags:\n",
    "    jieba.add_word(w, freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac869af",
   "metadata": {},
   "source": [
    "Sometimes you may want to add your own keywords, which may not be in `categories_and_tags`, you can add it to a text file.  In this case it is called `custom_dictionary.txt`.\n",
    "\n",
    "The format of this file is simple: one keyword in a row, with optional frequency and POS tag.  [More details here](https://github.com/fxsjy/jieba#%E8%BD%BD%E5%85%A5%E8%AF%8D%E5%85%B8)\n",
    "\n",
    "To use it you can run: `jieba.load_userdict(\"custom_dictionary.txt\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baf959",
   "metadata": {},
   "source": [
    "### This is the most time-consuming code\n",
    "\n",
    "For around 5,700 Chinese articles (words around 100 - 300 words), it takes around < 3 mins for TF-IDF to complete the tokenize process.\n",
    "\n",
    "For tqdm in pandas, [reference here](https://stackoverflow.com/a/34365537/1802483)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44badb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 44 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "\n",
    "jieba.analyse.set_stop_words(\"stop_words.txt\")\n",
    "topK = 10\n",
    "all_data['title_content_keywords_tfidf'] = all_data['title_content'].progress_apply(lambda x: jieba.analyse.extract_tags(x, topK=topK) )\n",
    "\n",
    "# This is slow\n",
    "# all_data['title_content_keywords_textrank'] = all_data['title_content'].progress_apply(lambda x: jieba.analyse.textrank(x, topK=topK) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d29811",
   "metadata": {},
   "source": [
    "Let's pick an article and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58bffb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['枕頭', '選擇', '脊醫', '打鼻鼾', '合適', '頸部', '蕎麥', '適合', '時間', '睡覺']\n"
     ]
    }
   ],
   "source": [
    "print(all_data['title_content_keywords_tfidf'][1])\n",
    "# print(all_data['title_content_keywords_textrank'][112])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85c05d",
   "metadata": {},
   "source": [
    "Combine/Join keywords generated from `jieba.analyse.extract_tags()` and keywords from `all_data['keywords']`\n",
    "\n",
    "Update: It seems that `jieba.analyse.textrank()` generates not so good result, so I did not use `title_content_keywords_textrank` here.  If you want to add it back, you can do this:\n",
    "\n",
    "`all_data['bow_tfidf_kw'] = all_data['title_content_keywords_tfidf'] + all_data['title_content_keywords_textrank'] + all_data['keywords']`\n",
    "\n",
    "Beware: Running `jieba.analyse.textrank()` is slow.  It usually takes around 3 hours to complete the whole progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f42983d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['bow_tfidf_kw'] = all_data['title_content_keywords_tfidf'] + all_data['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9df1c",
   "metadata": {},
   "source": [
    "Since the above will generate a `list` inside `Series`, so I convert the `list` to plain string inside `Series`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22827ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data['bow_to_str'] = all_data['bow_tfidf_kw'].progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08106b4d",
   "metadata": {},
   "source": [
    "## Congratulation! You finally finish Step 1!\n",
    "\n",
    "And yes!  This is how time-consuming and so important in all machine learning/AI/Data science project when you are preprocess/preparing your data.\n",
    "\n",
    "So don't think that machine-learning/AI/Data Science is all about algorithm and is the most important part.  It is not.\n",
    "\n",
    "Think about this: if you have the perfect algorithm in the world, and you have wrong data, it will produce wrong result.  Period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba12c52",
   "metadata": {},
   "source": [
    "## Step 2: Count the occurences of each words using CountVectorizer()\n",
    "\n",
    "Hold on, we move fast here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb22317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer()\n",
    "cv_mx = cv.fit_transform(all_data['bow_to_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8b537",
   "metadata": {},
   "source": [
    "## Step 3: Use the output from above and calculate the similarities using cosine_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7b19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(cv_mx, cv_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcc59b",
   "metadata": {},
   "source": [
    "**Sidenote: Backup and zip the old cosine data file called `models/model_YYMMDDHHII.zip`, then create the latest one called `models/model_latest.npy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7a7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 47.2 s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "backup_file = f\"models/model_{date_time}.zip\"\n",
    "model_file = \"models/model_latest.npy\" # .npy will be appended if it is not ended with .npy\n",
    "\n",
    "with ZipFile(backup_file, \"w\", ZIP_DEFLATED, compresslevel=9) as z:\n",
    "    z.write(model_file)\n",
    "\n",
    "# Remove old file\n",
    "if os.path.exists(model_file):\n",
    "    os.remove(model_file)\n",
    "\n",
    "# Write new filen\n",
    "np.save(model_file, cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6733cc0",
   "metadata": {},
   "source": [
    "### Pause here for a moment...\n",
    "\n",
    "The processing of creating a \"Model\" is actually completed here.  Below steps are for generating result using the file created above.\n",
    "\n",
    "In real project, we usually create a cron/schedule job to generate the model file.  Then we create an API to:\n",
    "\n",
    "- Read the model file\n",
    "- Get the result by passing in existing article title\n",
    "\n",
    "So what we did below have to be considered as a separated API, which mean **it did not have any coding relationship with above.**\n",
    "\n",
    "I just put the code below for your reference and how it works only.\n",
    "\n",
    "**Repeat: In real life project, the code below would be a separated API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add2ad2",
   "metadata": {},
   "source": [
    "#### Load the model from file and get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65841162",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = np.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cdc8863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=3, step=1)\n",
      "<class 'pandas.core.indexes.range.RangeIndex'>\n"
     ]
    }
   ],
   "source": [
    "print(all_data.index)\n",
    "print(type(all_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b820d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            貓狗不舒服  主人要知道！ 洞悉10大常見疾病\n",
      "1    鼻鼾改善｜脊醫教你選擇枕頭4大重點 有助改善疼痛、打鼻鼾情況！\n",
      "2        精神健康｜明明好攰 點解都係瞓得唔好？失眠原因知多少！\n",
      "Name: post_title, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(all_data['post_title'])\n",
    "print(type(all_data['post_title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c77ce",
   "metadata": {},
   "source": [
    "### Create list of indices for later matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b6ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(all_data.index, index = all_data['post_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d058c2",
   "metadata": {},
   "source": [
    "## Step 4: Create a function to return the top N similar topics using the topic/title as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ca16a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_article(title, n = 10, cosine_sim = cosine_sim):\n",
    "    # retrieve matching movie title index\n",
    "    if title not in indices.index:\n",
    "        print(\"Article not found\")\n",
    "        return\n",
    "    else:\n",
    "        idx = indices[title]\n",
    "    \n",
    "    # cosine similarity scores of movies in descending order\n",
    "    scores = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "    \n",
    "    # top n most similar movies indexes\n",
    "    # use 1:n because 0 is the same movie entered\n",
    "    top_n_idx = list(scores.iloc[1:n].index)\n",
    "        \n",
    "    return all_data['post_title'].iloc[top_n_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b84738",
   "metadata": {},
   "source": [
    "## Test your machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00fe5b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    鼻鼾改善｜脊醫教你選擇枕頭4大重點 有助改善疼痛、打鼻鼾情況！\n",
       "2        精神健康｜明明好攰 點解都係瞓得唔好？失眠原因知多少！\n",
       "Name: post_title, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_article(\"貓狗不舒服  主人要知道！ 洞悉10大常見疾病\", n = 10, cosine_sim = cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
